{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement des tweets: NLP (Natural LanguageProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Fouille de Données\n",
    "# Thème : Classification et clustering des tweets en Python.\n",
    "<h1> Réaliser par : Hajer Chakroun </h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objectifs : \n",
    "* Maitriser l’API de twitter pour l’extraction des tweets\n",
    "1. Maitriser la partie NLP (natural language processing) avec NLTK en Python\n",
    "2. Appliquer les principes de nettoyage des données\n",
    "3. Classer les tweets : regrouper ensemble les tweets qui sont similaires. C’est une étape qui peut être considérée comme une étape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Twitter\n",
    "\n",
    "Twitter est un réseau social de microblogage géré par l'entreprise Twitter Inc. Il permet à un utilisateur d’envoyer gratuitement de brefs messages, appelés tweets, sur internet, par messagerie instantanée ou par SMS. Ces messages sont limités à 280 caractères\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Spécifications\n",
    "\n",
    "Imaginons que vous avez un compte Twitter, et que vous lez suivre les tweets (texte très court) sur ce\n",
    "réseau social. Vu le nombre colossal de Tweets, et faute de temps, vous n’avez pas la possibilité de les\n",
    "lire tous. Pour cela, vous avez besoin d’une application qui va jouer le rôle d’assistantet qui va vous\n",
    "effectuer un résumé de toutes ces informations. Une des approches qu’on peut utiliser estde le classer\n",
    "sous former de groupes de sorte à ce qu’on présente à l’utilisateur un seul Tweet de chaque groupe.\n",
    "Pour cela, on doit procéder en trois grandes étapes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Prétraitement des tweets </h3>\n",
    "Dans cette étape, l’objectif est d’éliminer le texte inutile des tweets tels que les #, les noms des\n",
    "utilisateurs, les url, …\n",
    "\n",
    "<h3> 2. Traitement des tweets : NLP (Natural LanguageProcessing)</h3>\n",
    "On doit procéder à l’analyse du tweet en respectant les différentes étapes du NLP (Natural\n",
    "LanguageProcessing). La bibliothèque à utiliser est NLTK en Python.\n",
    "\n",
    "<h3> 3. Classification des tweets </h3>\n",
    "Etant donné un ensemble de tweets, l’objectif est de les résumer sous formes de groupes de sorte à\n",
    "ce que les Tweets qui sont dans le même groupe soient similaires. Ainsi, l’utilisateur pourra par la\n",
    "suite lire juste un Tweet de chaque groupe (le Tweet qui est le centroïde de groupes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réalisation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Les bibliothèques utilisées seront ajoutées au fur et à mesure tout au long du projet dans cette partie du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\pc\\anaconda\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: spacy in c:\\users\\pc\\anaconda\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (7.4.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm in c:\\users\\pc\\anaconda\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from en_core_web_sm) (2.3.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (4.47.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (0.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (49.2.0.post20200714)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (2.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (7.4.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.18.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.0.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import datetime\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Pour créer notre dataset on a besoin de télécharger des tweets à partir de Twitter en utilisant l’API de twitter. Pour cela, on a créé un compte «Twitter Développer», et on a eu une clé secrète d'API et autres de tokens qu'on va les utiliser. \n",
    "![alt text](output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('dQAqgwCB9saD67dEOzji0PQuE', 'BQbsZ6SpG91ESkaoqiug65L29oHHUqPGB2dTmm7B910EEPuqyP')\n",
    "auth.set_access_token('1324987221087301632-ebJRIlmfvMx1UeVm45I1HZA34wPwt8', 'j7zGNCLz9ZPB47wVXVC0Ripg6RNpwzG2dzTwNXOoJUsTI')\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On va sauvegarder les tweets dans un fichier .csv qu'on l'appelle twitter_data_analysisY-M-D-H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filename = \\'Datasets/twitter_data_analysis\\'+(datetime.datetime.now().strftime(\"%Y-%m-%d-%H\"))+\\'.csv\\'\\nwith open (filename, \\'w\\', newline=\\'\\',encoding=\"utf-8\") as csvFile:\\n    csvWriter = csv.writer(csvFile)\\n    csvWriter.writerow([\\'date\\', \\'TweetId\\',\\'Tweet\\',\\'created_at\\',\\'geo\\',\\'place\\',\\'coordinates\\',\\'location\\'])\\n    #using tweepy Cursor\\n    for tweet in tweepy.Cursor(api.user_timeline , id=\"Twitter\").items(11000):\\n        csvWriter.writerow([datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M\"), tweet.id, tweet.text, tweet.created_at, tweet.geo, tweet.place.name if tweet.place else None, tweet.coordinates, tweet._json[\"user\"][\"location\"]])\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''filename = 'Datasets/twitter_data_analysis'+(datetime.datetime.now().strftime(\"%Y-%m-%d-%H\"))+'.csv'\n",
    "with open (filename, 'w', newline='',encoding=\"utf-8\") as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['date', 'TweetId','Tweet','created_at','geo','place','coordinates','location'])\n",
    "    #using tweepy Cursor\n",
    "    for tweet in tweepy.Cursor(api.user_timeline , id=\"Twitter\").items(11000):\n",
    "        csvWriter.writerow([datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M\"), tweet.id, tweet.text, tweet.created_at, tweet.geo, tweet.place.name if tweet.place else None, tweet.coordinates, tweet._json[\"user\"][\"location\"]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pour obtenir au moins 10 mille tweets, on a concaténé 4 fichiers qui sont chargés auparavant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (12890, 8)\n",
      "Columns are: Index(['date', 'TweetId', 'Tweet', 'created_at', 'geo', 'place', 'coordinates',\n",
      "       'location'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12890 entries, 0 to 12889\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   date         12890 non-null  object \n",
      " 1   TweetId      12890 non-null  int64  \n",
      " 2   Tweet        12890 non-null  object \n",
      " 3   created_at   12890 non-null  object \n",
      " 4   geo          0 non-null      float64\n",
      " 5   place        264 non-null    object \n",
      " 6   coordinates  0 non-null      float64\n",
      " 7   location     12890 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 805.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>2020-11-19 23:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>2020-11-19 00:16:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>2020-11-19 00:14:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>2020-11-18 17:02:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>2020-11-18 16:50:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet           created_at  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...  2020-11-19 23:05:00   \n",
       "1                                   @CloudNaii 40404  2020-11-19 00:16:53   \n",
       "2    @issahairplug drink water replaced good morning  2020-11-19 00:14:37   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets  2020-11-18 17:02:21   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...  2020-11-18 16:50:52   \n",
       "\n",
       "   geo place  coordinates    location  \n",
       "0  NaN   NaN          NaN  everywhere  \n",
       "1  NaN   NaN          NaN  everywhere  \n",
       "2  NaN   NaN          NaN  everywhere  \n",
       "3  NaN   NaN          NaN  everywhere  \n",
       "4  NaN   NaN          NaN  everywhere  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df1= pd.read_csv('Datasets/twitter_data_analysis2020-11-22-13.csv')\n",
    "tweet_df2= pd.read_csv('Datasets/twitter_data_analysis2020-12-13-11.csv')\n",
    "tweet_df3= pd.read_csv('Datasets/twitter_data_analysis2020-12-10-12.csv')\n",
    "tweet_df4= pd.read_csv('Datasets/twitter_data_analysis2020-12-14-14.csv')\n",
    "'''tweet_dfi= pd.concat([tweet_df1, tweet_df2], ignore_index= True)\n",
    "tweet_dfii= pd.concat([tweet_dfi, tweet_df3], ignore_index= True)'''\n",
    "tweet_df= pd.concat([tweet_df1, tweet_df2, tweet_df3, tweet_df4], ignore_index= True)\n",
    "\n",
    "# Affichage de la taille du dataset (n_lignes and n_colonnes)\n",
    "print('Dataset size:',tweet_df.shape)\n",
    "print('Columns are:',tweet_df.columns)\n",
    "tweet_df.info()\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \n",
       "0  RT @shesooosaddity: if you had a twitter befor...  \n",
       "1                                   @CloudNaii 40404  \n",
       "2    @issahairplug drink water replaced good morning  \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets  \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df=tweet_df.drop(columns = ['created_at','geo','place', 'coordinates', 'location'])\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prétraitement\n",
    "Après avoir observé les données, nous avons vu que les phrases contenaient des balises HTML, des mots-vides et toute la ponctuation. Nous avons donc commencé par éliminer le bruit pour normaliser nos phrases.\n",
    "Nous supprimons les balises HTML, nous supprimons aussi tous les caractères qui ne sont pas des lettres et donc, supprimons toute la ponctuation des textes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les hashtags, les mentions, les punctuations et les caractères indésirables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104643062902788</td>\n",
       "      <td>@ambr_ncole they're tourists</td>\n",
       "      <td>ambrncole theyre tourists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329101613940797441</td>\n",
       "      <td>@PhallonXOXO proof you're doing it right 😌</td>\n",
       "      <td>PhallonXOXO proof youre doing it right 😌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328838299419627525</td>\n",
       "      <td>some of you hating...\\n\\nbut we see you Fleeti...</td>\n",
       "      <td>some of you hating\\n\\nbut we see you Fleeting 🧐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328684389388185600</td>\n",
       "      <td>That thing you didn’t Tweet but wanted to but ...</td>\n",
       "      <td>That thing you didn’t Tweet but wanted to but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328426768009736192</td>\n",
       "      <td>@quakerraina this is art</td>\n",
       "      <td>quakerraina this is art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "5  2020-11-22  13:18  1329104643062902788   \n",
       "6  2020-11-22  13:18  1329101613940797441   \n",
       "7  2020-11-22  13:18  1328838299419627525   \n",
       "8  2020-11-22  13:18  1328684389388185600   \n",
       "9  2020-11-22  13:18  1328426768009736192   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "5                       @ambr_ncole they're tourists   \n",
       "6         @PhallonXOXO proof you're doing it right 😌   \n",
       "7  some of you hating...\\n\\nbut we see you Fleeti...   \n",
       "8  That thing you didn’t Tweet but wanted to but ...   \n",
       "9                           @quakerraina this is art   \n",
       "\n",
       "                                         Tweet_punct  \n",
       "0  RT shesooosaddity if you had a twitter before ...  \n",
       "1                                         CloudNaii   \n",
       "2     issahairplug drink water replaced good morning  \n",
       "3           NeThatGuy were taking oomf to the Fleets  \n",
       "4         JusJust remember I dedicate my th Tweet to  \n",
       "5                          ambrncole theyre tourists  \n",
       "6           PhallonXOXO proof youre doing it right 😌  \n",
       "7    some of you hating\\n\\nbut we see you Fleeting 🧐  \n",
       "8  That thing you didn’t Tweet but wanted to but ...  \n",
       "9                            quakerraina this is art  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweet_df['Tweet_punct'] = tweet_df['Tweet'].apply(lambda x: remove_punct(x))\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les lettres sont également passées en minuscule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "\n",
       "                                     Tweet_tokenized  \n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...  \n",
       "1                                      [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replaced, good, m...  \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]  \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split(' ', text)\n",
    "    return text\n",
    "tweet_df['Tweet_tokenized'] = tweet_df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pc\\anaconda\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parce que les mots-vides, par définition, n’apportent pas d’information au texte, nous les éliminons aussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword.extend(['a’s', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards',\\\n",
    "                 'again', 'against', 'ain’t', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already','also', 'although', 'always', 'am', 'among',\\\n",
    "                 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways',\\\n",
    "                 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'aren’t', 'around', 'as', 'aside', 'ask', 'asking',\\\n",
    "                 'associated', 'at', 'available', 'away', 'awfully', 'be', 'became', 'because', 'become', 'becomes', 'becoming',\\\n",
    "                 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside','besides', 'best', 'better', 'between',\\\n",
    "                 'beyond', 'both', 'brief', 'but', 'by', 'c’mon', 'c’s', 'came', 'can', 'can’t', 'cannot', 'cant', 'cause', 'causes',\\\n",
    "                 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'old', 'new', 'age', 'lot', 'bag', 'top', 'cat', 'bat', 'sap', 'jda', 'tea', 'dog', 'lie', 'law', 'lab',\\\n",
    "                 'mob', 'map', 'car', 'fat', 'sea', 'saw', 'raw', 'rob', 'win', 'can', 'get', 'fan', 'fun', 'big',\\\n",
    "                 'use', 'pea', 'pit','pot', 'pat', 'ear', 'eye', 'kit', 'pot', 'pen', 'bud', 'bet', 'god', 'tax', 'won', 'run',\\\n",
    "                 'lid', 'log', 'pr', 'pd', 'cop', 'nyc', 'ny', 'la', 'toy', 'war', 'law', 'lax', 'jfk', 'fed', 'cry', 'ceo',\\\n",
    "                 'pay', 'pet', 'fan', 'fun', 'usd', 'rio',':)', ';)', '(:', '(;', '}', '{','}','here', 'there', 'where', 'when', 'would', 'should', 'could','thats', 'youre', 'thanks', 'hasn',\\\n",
    "                 'thank', 'https', 'since', 'wanna', 'gonna', 'aint', 'http', 'unto', 'onto', 'into', 'havent',\\\n",
    "                 'dont', 'done', 'cant', 'werent', 'https', 'u', 'isnt', 'go', 'theyre', 'each', 'every', 'shes', 'youve', 'youll',\\\n",
    "                 'weve', 'theyve','googleele' , 'goog', 'lyin', 'lie', 'googles', 'goog', 'aapl','apple',\\\n",
    "                 'msft','microsoft', 'google', 'goog', 'googl','goog','https', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains',\\\n",
    "                 'corresponding', 'could', 'couldn’t', 'course', 'currently', 'definitely', 'described', 'despite',\\\n",
    "                 'did', 'didn’t', 'different', 'do', 'does', 'doesn’t', 'doing', 'don’t', 'done', 'down', 'downwards', 'during', 'each', 'edu', 'eg', 'eight', 'either',\\\n",
    "                 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything',\\\n",
    "                 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows',\\\n",
    "                 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'go',\\\n",
    "                 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had', 'hadn’t', 'happens', 'hardly', 'has', 'hasn’t', 'have', 'haven’t',\\\n",
    "                 'having', 'he', 'he’s', 'hello', 'help', 'hence', 'her', 'here', 'here’s', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself',\\\n",
    "                 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i’d', 'i’ll', 'i’m', 'i’ve', 'ie', 'if', 'ignored', 'immediate',\\\n",
    "                 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'isn’t', 'it', 'it’d', 'it’ll',\\\n",
    "                 'it’s', 'its', 'itself', 'just', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'last', 'lately', 'later', 'latter', 'latterly',\\\n",
    "                 'least', 'less', 'lest', 'let', 'let’s', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'mainly', 'many', 'may', 'maybe',\\\n",
    "                 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary',\\\n",
    "                 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere',\\\n",
    "                 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own',\\\n",
    "                 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'que', 'quite', 'qv', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right',\\\n",
    "                 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she',\\\n",
    "                 'should', 'shouldn’t', 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying',\\\n",
    "                 'still', 'sub', 'such', 'sup', 'sure', 't’s', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'that’s', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'there’s', 'thereafter', 'thereby',\\\n",
    "                 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'they’d', 'they’ll', 'they’re', 'they’ve', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards',\\\n",
    "                 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon',\\\n",
    "                 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'value', 'various', 'very', 'via', 'viz', 'vs', 'want', 'wants', 'was', 'wasn’t', 'way',\\\n",
    "                 'we', 'we’d', 'we’ll', 'we’re', 'we’ve', 'welcome', 'well', 'went', 'were', 'weren’t', 'what', 'what’s', 'whatever', 'when', 'whence', 'whenever', 'where', 'where’s', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither',\\\n",
    "                 'who', 'who’s', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'won’t', 'wonder', 'would', 'wouldn’t', 'yes', 'yet',\\\n",
    "                 'you', 'you’d', 'you’ll', 'you’re', 'you’ve', 'your', 'yours', 'yourself', 'yourselves', 'zero'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les mots qui n'expriment aucun sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104643062902788</td>\n",
       "      <td>@ambr_ncole they're tourists</td>\n",
       "      <td>ambrncole theyre tourists</td>\n",
       "      <td>[ambrncole, theyre, tourists]</td>\n",
       "      <td>[ambrncole, tourists]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329101613940797441</td>\n",
       "      <td>@PhallonXOXO proof you're doing it right 😌</td>\n",
       "      <td>PhallonXOXO proof youre doing it right 😌</td>\n",
       "      <td>[phallonxoxo, proof, youre, doing, it, right, 😌]</td>\n",
       "      <td>[phallonxoxo, proof, 😌]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328838299419627525</td>\n",
       "      <td>some of you hating...\\n\\nbut we see you Fleeti...</td>\n",
       "      <td>some of you hating\\n\\nbut we see you Fleeting 🧐</td>\n",
       "      <td>[some, of, you, hating\\n\\nbut, we, see, you, f...</td>\n",
       "      <td>[hating\\n\\nbut, fleeting, 🧐]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328684389388185600</td>\n",
       "      <td>That thing you didn’t Tweet but wanted to but ...</td>\n",
       "      <td>That thing you didn’t Tweet but wanted to but ...</td>\n",
       "      <td>[that, thing, you, didn’t, tweet, but, wanted,...</td>\n",
       "      <td>[thing, tweet, wanted, close, nah, \\n\\nwe, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328426768009736192</td>\n",
       "      <td>@quakerraina this is art</td>\n",
       "      <td>quakerraina this is art</td>\n",
       "      <td>[quakerraina, this, is, art]</td>\n",
       "      <td>[quakerraina, art]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "5  2020-11-22  13:18  1329104643062902788   \n",
       "6  2020-11-22  13:18  1329101613940797441   \n",
       "7  2020-11-22  13:18  1328838299419627525   \n",
       "8  2020-11-22  13:18  1328684389388185600   \n",
       "9  2020-11-22  13:18  1328426768009736192   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "5                       @ambr_ncole they're tourists   \n",
       "6         @PhallonXOXO proof you're doing it right 😌   \n",
       "7  some of you hating...\\n\\nbut we see you Fleeti...   \n",
       "8  That thing you didn’t Tweet but wanted to but ...   \n",
       "9                           @quakerraina this is art   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "5                          ambrncole theyre tourists   \n",
       "6           PhallonXOXO proof youre doing it right 😌   \n",
       "7    some of you hating\\n\\nbut we see you Fleeting 🧐   \n",
       "8  That thing you didn’t Tweet but wanted to but ...   \n",
       "9                            quakerraina this is art   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]   \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...   \n",
       "5                      [ambrncole, theyre, tourists]   \n",
       "6   [phallonxoxo, proof, youre, doing, it, right, 😌]   \n",
       "7  [some, of, you, hating\\n\\nbut, we, see, you, f...   \n",
       "8  [that, thing, you, didn’t, tweet, but, wanted,...   \n",
       "9                       [quakerraina, this, is, art]   \n",
       "\n",
       "                                       Tweet_nonstop  \n",
       "0                [rt, shesooosaddity, twitter, , rt]  \n",
       "1                                      [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replaced, good, m...  \n",
       "3                  [nethatguy, taking, oomf, fleets]  \n",
       "4               [jusjust, remember, dedicate, tweet]  \n",
       "5                              [ambrncole, tourists]  \n",
       "6                            [phallonxoxo, proof, 😌]  \n",
       "7                       [hating\\n\\nbut, fleeting, 🧐]  \n",
       "8  [thing, tweet, wanted, close, nah, \\n\\nwe, pla...  \n",
       "9                                 [quakerraina, art]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tweet_df['Tweet_nonstop'] = tweet_df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK est une suite de bibliothèques de traitement de texte pour la classification, la tokenisation, la recherche de racines, le marquage, l'analyse et le raisonnement sémantique.\n",
    "On vas utiliser la bibliothèque NLTK pour effectuer une analyse de chaque tweet et le transformer en un\n",
    "ensemble de mots en suivant les différentes étapes de base du processus NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming et Lammitization est une technique utilisée pour extraire la forme de base des mots en supprimant les affixes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "      <td>[rt, shesooosadd, twitter, , rt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replac, good, morn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleets]</td>\n",
       "      <td>[nethatguy, take, oomf, fleet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "      <td>[jusjust, rememb, dedic, tweet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]   \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...   \n",
       "\n",
       "                                       Tweet_nonstop  \\\n",
       "0                [rt, shesooosaddity, twitter, , rt]   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3                  [nethatguy, taking, oomf, fleets]   \n",
       "4               [jusjust, remember, dedicate, tweet]   \n",
       "\n",
       "                                      Tweet_stemmed  \n",
       "0                  [rt, shesooosadd, twitter, , rt]  \n",
       "1                                     [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replac, good, morn]  \n",
       "3                    [nethatguy, take, oomf, fleet]  \n",
       "4                   [jusjust, rememb, dedic, tweet]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweet_df['Tweet_stemmed'] = tweet_df['Tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "      <td>[rt, shesooosadd, twitter, , rt]</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replac, good, morn]</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleets]</td>\n",
       "      <td>[nethatguy, take, oomf, fleet]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "      <td>[jusjust, rememb, dedic, tweet]</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]   \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...   \n",
       "\n",
       "                                       Tweet_nonstop  \\\n",
       "0                [rt, shesooosaddity, twitter, , rt]   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3                  [nethatguy, taking, oomf, fleets]   \n",
       "4               [jusjust, remember, dedicate, tweet]   \n",
       "\n",
       "                                      Tweet_stemmed  \\\n",
       "0                  [rt, shesooosadd, twitter, , rt]   \n",
       "1                                     [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replac, good, morn]   \n",
       "3                    [nethatguy, take, oomf, fleet]   \n",
       "4                   [jusjust, rememb, dedic, tweet]   \n",
       "\n",
       "                                    Tweet_lemmatized  \n",
       "0                [rt, shesooosaddity, twitter, , rt]  \n",
       "1                                      [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replaced, good, m...  \n",
       "3                   [nethatguy, taking, oomf, fleet]  \n",
       "4               [jusjust, remember, dedicate, tweet]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweet_df['Tweet_lemmatized'] = tweet_df['Tweet_nonstop'].apply(lambda x: lemmatizer(x))\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Les données après le prétraitement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      [rt, shesooosaddity, twitter, , rt]\n",
       "1                                            [cloudnaii, ]\n",
       "2        [issahairplug, drink, water, replaced, good, m...\n",
       "3                         [nethatguy, taking, oomf, fleet]\n",
       "4                     [jusjust, remember, dedicate, tweet]\n",
       "                               ...                        \n",
       "12885               [themegaboi, keeping, brain, thinking]\n",
       "12886    [guillaumetc, hamillhimself, chrisevans, combo...\n",
       "12887                                   [ksjize, dogrates]\n",
       "12888               [insomniacookies, cc, mecookiemonster]\n",
       "12889    [mnoir, amp, guaranteed, good, morning, good, ...\n",
       "Name: Tweet_lemmatized, Length: 12890, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.Tweet_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On va enregistrer notre dataset prétraité dans un nouveau fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.Tweet_lemmatized.to_csv('Datasets/new_cleaned_tweets.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (12890, 1)\n",
      "Columns are: Index(['Tweet_lemmatized'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12890 entries, 0 to 12889\n",
      "Data columns (total 1 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Tweet_lemmatized  12890 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 100.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['rt', 'shesooosaddity', 'twitter', '', 'rt']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cloudnaii', '']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['issahairplug', 'drink', 'water', 'replaced',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nethatguy', 'taking', 'oomf', 'fleet']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['jusjust', 'remember', 'dedicate', 'tweet']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Tweet_lemmatized\n",
       "0      ['rt', 'shesooosaddity', 'twitter', '', 'rt']\n",
       "1                                  ['cloudnaii', '']\n",
       "2  ['issahairplug', 'drink', 'water', 'replaced',...\n",
       "3           ['nethatguy', 'taking', 'oomf', 'fleet']\n",
       "4       ['jusjust', 'remember', 'dedicate', 'tweet']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affichage\n",
    "new_tweet_df= pd.read_csv('Datasets/new_cleaned_tweets.csv')\n",
    "print('Dataset size:',new_tweet_df.shape)\n",
    "print('Columns are:',new_tweet_df.columns)\n",
    "new_tweet_df.info()\n",
    "new_tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation\n",
    "Les données nettoyées devient sur une seule ligne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4591)\t2\n",
      "  (0, 4803)\t1\n",
      "  (0, 5616)\t1\n",
      "  (1, 899)\t1\n",
      "  (2, 2722)\t1\n",
      "  (2, 1340)\t1\n",
      "  (2, 5816)\t1\n",
      "  (2, 4480)\t1\n",
      "  (2, 1859)\t1\n",
      "  (2, 3637)\t1\n",
      "  (3, 3772)\t1\n",
      "  (3, 5242)\t1\n",
      "  (3, 4027)\t1\n",
      "  (3, 1687)\t1\n",
      "  (4, 2958)\t1\n",
      "  (4, 4468)\t1\n",
      "  (4, 1185)\t1\n",
      "  (4, 5603)\t1\n",
      "  (5, 172)\t1\n",
      "  (5, 5526)\t1\n",
      "  (6, 4167)\t1\n",
      "  (6, 4307)\t1\n",
      "  (7, 1994)\t1\n",
      "  (7, 3745)\t1\n",
      "  (7, 1688)\t1\n",
      "  :\t:\n",
      "  (12882, 3461)\t1\n",
      "  (12883, 2587)\t1\n",
      "  (12884, 369)\t1\n",
      "  (12884, 3607)\t1\n",
      "  (12885, 597)\t1\n",
      "  (12885, 5407)\t1\n",
      "  (12885, 3008)\t1\n",
      "  (12885, 5362)\t1\n",
      "  (12886, 1931)\t1\n",
      "  (12886, 1955)\t1\n",
      "  (12886, 844)\t1\n",
      "  (12886, 941)\t1\n",
      "  (12886, 5863)\t1\n",
      "  (12887, 1297)\t1\n",
      "  (12887, 3092)\t1\n",
      "  (12888, 3474)\t1\n",
      "  (12888, 758)\t1\n",
      "  (12888, 2680)\t1\n",
      "  (12889, 1859)\t2\n",
      "  (12889, 3637)\t1\n",
      "  (12889, 5603)\t1\n",
      "  (12889, 3865)\t1\n",
      "  (12889, 179)\t1\n",
      "  (12889, 3598)\t1\n",
      "  (12889, 1925)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X=cv.fit_transform(new_tweet_df.Tweet_lemmatized)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des tweets\n",
    "\n",
    "On va utiliser l’algorithme K-Means pour classer les Tweets en 30 classes, qui est un algorithme non supervisé  de clustering, populaire en Machine Learning qui peuvent classer chaque tweet à une catégorie particulière ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:fitting model for 3 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 4 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 5 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 6 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 7 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 8 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 9 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 10 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 11 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 12 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 13 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 14 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "seed = 42\n",
    "\n",
    "ks = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "# track a couple of metrics\n",
    "sil_scores = []\n",
    "result= []\n",
    "\n",
    "# fit the models, save the evaluation metrics from each run\n",
    "for k in ks:\n",
    "    logging.warning('fitting model for {} clusters'.format(k))\n",
    "    Kmeans = KMeans(n_clusters=k, n_jobs=-1, random_state=seed)\n",
    "    Kmeans.fit(X)\n",
    "    labels = Kmeans.labels_\n",
    "    #sil_scores.append(silhouette_score(bio_matrix, labels))\n",
    "    result.append(Kmeans.inertia_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, result, 'o--')\n",
    "plt.ylabel('result')\n",
    "plt.title('kmeans parameter search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks,result)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('word per cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k=30\n",
    "Kmeans=KMeans(n_clusters=true_k,init='k-means++',n_init=1)\n",
    "Kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = Kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    print(\"-----------------------\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour chaque cluster on va afficher un seul tweet \n",
    "On va afficher le tweet qui a le plus grand score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "while i<28:\n",
    "    while True: \n",
    "        Y=cv.transform([new_tweet_df.Tweet_lemmatized[j]])\n",
    "        prediction=Kmeans.predict(Y)\n",
    "        if i == prediction:\n",
    "            print(\"Tweet of cluster \"+str(prediction)+\" : \"+tweet_df.Tweet[i])\n",
    "            print (\"-----------------------------------------------\")\n",
    "            print(\"\\n\")\n",
    "            j=0\n",
    "            break\n",
    "        j+=1\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
