{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement des tweets: NLP (Natural LanguageProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Fouille de Donn√©es\n",
    "# Th√®me : Classification et clustering des tweets en Python.\n",
    "<h1> R√©aliser par : Hajer Chakroun </h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objectifs : \n",
    "* Maitriser l‚ÄôAPI de twitter pour l‚Äôextraction des tweets\n",
    "1. Maitriser la partie NLP (natural language processing) avec NLTK en Python\n",
    "2. Appliquer les principes de nettoyage des donn√©es\n",
    "3. Classer les tweets : regrouper ensemble les tweets qui sont similaires. C‚Äôest une √©tape qui peut √™tre consid√©r√©e comme une √©tape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Twitter\n",
    "\n",
    "Twitter est un r√©seau social de microblogage g√©r√© par l'entreprise Twitter Inc. Il permet √† un utilisateur d‚Äôenvoyer gratuitement de brefs messages, appel√©s tweets, sur internet, par messagerie instantan√©e ou par SMS. Ces messages sont limit√©s √† 280 caract√®res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sp√©cifications\n",
    "\n",
    "Imaginons que vous avez un compte Twitter, et que vous lez suivre les tweets (texte tr√®s court) sur ce\n",
    "r√©seau social. Vu le nombre colossal de Tweets, et faute de temps, vous n‚Äôavez pas la possibilit√© de les\n",
    "lire tous. Pour cela, vous avez besoin d‚Äôune application qui va jouer le r√¥le d‚Äôassistantet qui va vous\n",
    "effectuer un r√©sum√© de toutes ces informations. Une des approches qu‚Äôon peut utiliser estde le classer\n",
    "sous former de groupes de sorte √† ce qu‚Äôon pr√©sente √† l‚Äôutilisateur un seul Tweet de chaque groupe.\n",
    "Pour cela, on doit proc√©der en trois grandes √©tapes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Pr√©traitement des tweets </h3>\n",
    "Dans cette √©tape, l‚Äôobjectif est d‚Äô√©liminer le texte inutile des tweets tels que les #, les noms des\n",
    "utilisateurs, les url, ‚Ä¶\n",
    "\n",
    "<h3> 2. Traitement des tweets : NLP (Natural LanguageProcessing)</h3>\n",
    "On doit proc√©der √† l‚Äôanalyse du tweet en respectant les diff√©rentes √©tapes du NLP (Natural\n",
    "LanguageProcessing). La biblioth√®que √† utiliser est NLTK en Python.\n",
    "\n",
    "<h3> 3. Classification des tweets </h3>\n",
    "Etant donn√© un ensemble de tweets, l‚Äôobjectif est de les r√©sumer sous formes de groupes de sorte √†\n",
    "ce que les Tweets qui sont dans le m√™me groupe soient similaires. Ainsi, l‚Äôutilisateur pourra par la\n",
    "suite lire juste un Tweet de chaque groupe (le Tweet qui est le centro√Øde de groupes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©alisation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Les biblioth√®ques utilis√©es seront ajout√©es au fur et √† mesure tout au long du projet dans cette partie du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\pc\\anaconda\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: spacy in c:\\users\\pc\\anaconda\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (7.4.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm in c:\\users\\pc\\anaconda\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from en_core_web_sm) (2.3.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (4.47.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (0.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (49.2.0.post20200714)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (2.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (7.4.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.18.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.0.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pc\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import datetime\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Pour cr√©er notre dataset on a besoin de t√©l√©charger des tweets √† partir de Twitter en utilisant l‚ÄôAPI de twitter. Pour cela, on a cr√©√© un compte ¬´Twitter D√©velopper¬ª, et on a eu une cl√© secr√®te d'API et autres de tokens qu'on va les utiliser. \n",
    "![alt text](output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('dQAqgwCB9saD67dEOzji0PQuE', 'BQbsZ6SpG91ESkaoqiug65L29oHHUqPGB2dTmm7B910EEPuqyP')\n",
    "auth.set_access_token('1324987221087301632-ebJRIlmfvMx1UeVm45I1HZA34wPwt8', 'j7zGNCLz9ZPB47wVXVC0Ripg6RNpwzG2dzTwNXOoJUsTI')\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On va sauvegarder les tweets dans un fichier .csv qu'on l'appelle twitter_data_analysisY-M-D-H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filename = \\'Datasets/twitter_data_analysis\\'+(datetime.datetime.now().strftime(\"%Y-%m-%d-%H\"))+\\'.csv\\'\\nwith open (filename, \\'w\\', newline=\\'\\',encoding=\"utf-8\") as csvFile:\\n    csvWriter = csv.writer(csvFile)\\n    csvWriter.writerow([\\'date\\', \\'TweetId\\',\\'Tweet\\',\\'created_at\\',\\'geo\\',\\'place\\',\\'coordinates\\',\\'location\\'])\\n    #using tweepy Cursor\\n    for tweet in tweepy.Cursor(api.user_timeline , id=\"Twitter\").items(11000):\\n        csvWriter.writerow([datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M\"), tweet.id, tweet.text, tweet.created_at, tweet.geo, tweet.place.name if tweet.place else None, tweet.coordinates, tweet._json[\"user\"][\"location\"]])\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''filename = 'Datasets/twitter_data_analysis'+(datetime.datetime.now().strftime(\"%Y-%m-%d-%H\"))+'.csv'\n",
    "with open (filename, 'w', newline='',encoding=\"utf-8\") as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['date', 'TweetId','Tweet','created_at','geo','place','coordinates','location'])\n",
    "    #using tweepy Cursor\n",
    "    for tweet in tweepy.Cursor(api.user_timeline , id=\"Twitter\").items(11000):\n",
    "        csvWriter.writerow([datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M\"), tweet.id, tweet.text, tweet.created_at, tweet.geo, tweet.place.name if tweet.place else None, tweet.coordinates, tweet._json[\"user\"][\"location\"]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pour obtenir au moins 10 mille tweets, on a concat√©n√© 4 fichiers qui sont charg√©s auparavant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (12890, 8)\n",
      "Columns are: Index(['date', 'TweetId', 'Tweet', 'created_at', 'geo', 'place', 'coordinates',\n",
      "       'location'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12890 entries, 0 to 12889\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   date         12890 non-null  object \n",
      " 1   TweetId      12890 non-null  int64  \n",
      " 2   Tweet        12890 non-null  object \n",
      " 3   created_at   12890 non-null  object \n",
      " 4   geo          0 non-null      float64\n",
      " 5   place        264 non-null    object \n",
      " 6   coordinates  0 non-null      float64\n",
      " 7   location     12890 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 805.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>2020-11-19 23:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>2020-11-19 00:16:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>2020-11-19 00:14:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>2020-11-18 17:02:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>2020-11-18 16:50:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everywhere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet           created_at  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...  2020-11-19 23:05:00   \n",
       "1                                   @CloudNaii 40404  2020-11-19 00:16:53   \n",
       "2    @issahairplug drink water replaced good morning  2020-11-19 00:14:37   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets  2020-11-18 17:02:21   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...  2020-11-18 16:50:52   \n",
       "\n",
       "   geo place  coordinates    location  \n",
       "0  NaN   NaN          NaN  everywhere  \n",
       "1  NaN   NaN          NaN  everywhere  \n",
       "2  NaN   NaN          NaN  everywhere  \n",
       "3  NaN   NaN          NaN  everywhere  \n",
       "4  NaN   NaN          NaN  everywhere  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df1= pd.read_csv('Datasets/twitter_data_analysis2020-11-22-13.csv')\n",
    "tweet_df2= pd.read_csv('Datasets/twitter_data_analysis2020-12-13-11.csv')\n",
    "tweet_df3= pd.read_csv('Datasets/twitter_data_analysis2020-12-10-12.csv')\n",
    "tweet_df4= pd.read_csv('Datasets/twitter_data_analysis2020-12-14-14.csv')\n",
    "'''tweet_dfi= pd.concat([tweet_df1, tweet_df2], ignore_index= True)\n",
    "tweet_dfii= pd.concat([tweet_dfi, tweet_df3], ignore_index= True)'''\n",
    "tweet_df= pd.concat([tweet_df1, tweet_df2, tweet_df3, tweet_df4], ignore_index= True)\n",
    "\n",
    "# Affichage de la taille du dataset (n_lignes and n_colonnes)\n",
    "print('Dataset size:',tweet_df.shape)\n",
    "print('Columns are:',tweet_df.columns)\n",
    "tweet_df.info()\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \n",
       "0  RT @shesooosaddity: if you had a twitter befor...  \n",
       "1                                   @CloudNaii 40404  \n",
       "2    @issahairplug drink water replaced good morning  \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets  \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df=tweet_df.drop(columns = ['created_at','geo','place', 'coordinates', 'location'])\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pr√©traitement\n",
    "Apr√®s avoir observ√© les donn√©es, nous avons vu que les phrases contenaient des balises HTML, des mots-vides et toute la ponctuation. Nous avons donc commenc√© par √©liminer le bruit pour normaliser nos phrases.\n",
    "Nous supprimons les balises HTML, nous supprimons aussi tous les caract√®res qui ne sont pas des lettres et donc, supprimons toute la ponctuation des textes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les hashtags, les mentions, les punctuations et les caract√®res ind√©sirables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104643062902788</td>\n",
       "      <td>@ambr_ncole they're tourists</td>\n",
       "      <td>ambrncole theyre tourists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329101613940797441</td>\n",
       "      <td>@PhallonXOXO proof you're doing it right üòå</td>\n",
       "      <td>PhallonXOXO proof youre doing it right üòå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328838299419627525</td>\n",
       "      <td>some of you hating...\\n\\nbut we see you Fleeti...</td>\n",
       "      <td>some of you hating\\n\\nbut we see you Fleeting üßê</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328684389388185600</td>\n",
       "      <td>That thing you didn‚Äôt Tweet but wanted to but ...</td>\n",
       "      <td>That thing you didn‚Äôt Tweet but wanted to but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328426768009736192</td>\n",
       "      <td>@quakerraina this is art</td>\n",
       "      <td>quakerraina this is art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "5  2020-11-22  13:18  1329104643062902788   \n",
       "6  2020-11-22  13:18  1329101613940797441   \n",
       "7  2020-11-22  13:18  1328838299419627525   \n",
       "8  2020-11-22  13:18  1328684389388185600   \n",
       "9  2020-11-22  13:18  1328426768009736192   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "5                       @ambr_ncole they're tourists   \n",
       "6         @PhallonXOXO proof you're doing it right üòå   \n",
       "7  some of you hating...\\n\\nbut we see you Fleeti...   \n",
       "8  That thing you didn‚Äôt Tweet but wanted to but ...   \n",
       "9                           @quakerraina this is art   \n",
       "\n",
       "                                         Tweet_punct  \n",
       "0  RT shesooosaddity if you had a twitter before ...  \n",
       "1                                         CloudNaii   \n",
       "2     issahairplug drink water replaced good morning  \n",
       "3           NeThatGuy were taking oomf to the Fleets  \n",
       "4         JusJust remember I dedicate my th Tweet to  \n",
       "5                          ambrncole theyre tourists  \n",
       "6           PhallonXOXO proof youre doing it right üòå  \n",
       "7    some of you hating\\n\\nbut we see you Fleeting üßê  \n",
       "8  That thing you didn‚Äôt Tweet but wanted to but ...  \n",
       "9                            quakerraina this is art  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweet_df['Tweet_punct'] = tweet_df['Tweet'].apply(lambda x: remove_punct(x))\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les lettres sont √©galement pass√©es en minuscule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "\n",
       "                                     Tweet_tokenized  \n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...  \n",
       "1                                      [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replaced, good, m...  \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]  \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split(' ', text)\n",
    "    return text\n",
    "tweet_df['Tweet_tokenized'] = tweet_df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pc\\anaconda\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda\\lib\\site-packages (from nltk) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parce que les mots-vides, par d√©finition, n‚Äôapportent pas d‚Äôinformation au texte, nous les √©liminons aussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword.extend(['a‚Äôs', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards',\\\n",
    "                 'again', 'against', 'ain‚Äôt', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already','also', 'although', 'always', 'am', 'among',\\\n",
    "                 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways',\\\n",
    "                 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'aren‚Äôt', 'around', 'as', 'aside', 'ask', 'asking',\\\n",
    "                 'associated', 'at', 'available', 'away', 'awfully', 'be', 'became', 'because', 'become', 'becomes', 'becoming',\\\n",
    "                 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside','besides', 'best', 'better', 'between',\\\n",
    "                 'beyond', 'both', 'brief', 'but', 'by', 'c‚Äômon', 'c‚Äôs', 'came', 'can', 'can‚Äôt', 'cannot', 'cant', 'cause', 'causes',\\\n",
    "                 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'old', 'new', 'age', 'lot', 'bag', 'top', 'cat', 'bat', 'sap', 'jda', 'tea', 'dog', 'lie', 'law', 'lab',\\\n",
    "                 'mob', 'map', 'car', 'fat', 'sea', 'saw', 'raw', 'rob', 'win', 'can', 'get', 'fan', 'fun', 'big',\\\n",
    "                 'use', 'pea', 'pit','pot', 'pat', 'ear', 'eye', 'kit', 'pot', 'pen', 'bud', 'bet', 'god', 'tax', 'won', 'run',\\\n",
    "                 'lid', 'log', 'pr', 'pd', 'cop', 'nyc', 'ny', 'la', 'toy', 'war', 'law', 'lax', 'jfk', 'fed', 'cry', 'ceo',\\\n",
    "                 'pay', 'pet', 'fan', 'fun', 'usd', 'rio',':)', ';)', '(:', '(;', '}', '{','}','here', 'there', 'where', 'when', 'would', 'should', 'could','thats', 'youre', 'thanks', 'hasn',\\\n",
    "                 'thank', 'https', 'since', 'wanna', 'gonna', 'aint', 'http', 'unto', 'onto', 'into', 'havent',\\\n",
    "                 'dont', 'done', 'cant', 'werent', 'https', 'u', 'isnt', 'go', 'theyre', 'each', 'every', 'shes', 'youve', 'youll',\\\n",
    "                 'weve', 'theyve','googleele' , 'goog', 'lyin', 'lie', 'googles', 'goog', 'aapl','apple',\\\n",
    "                 'msft','microsoft', 'google', 'goog', 'googl','goog','https', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains',\\\n",
    "                 'corresponding', 'could', 'couldn‚Äôt', 'course', 'currently', 'definitely', 'described', 'despite',\\\n",
    "                 'did', 'didn‚Äôt', 'different', 'do', 'does', 'doesn‚Äôt', 'doing', 'don‚Äôt', 'done', 'down', 'downwards', 'during', 'each', 'edu', 'eg', 'eight', 'either',\\\n",
    "                 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything',\\\n",
    "                 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows',\\\n",
    "                 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'go',\\\n",
    "                 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had', 'hadn‚Äôt', 'happens', 'hardly', 'has', 'hasn‚Äôt', 'have', 'haven‚Äôt',\\\n",
    "                 'having', 'he', 'he‚Äôs', 'hello', 'help', 'hence', 'her', 'here', 'here‚Äôs', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself',\\\n",
    "                 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i‚Äôd', 'i‚Äôll', 'i‚Äôm', 'i‚Äôve', 'ie', 'if', 'ignored', 'immediate',\\\n",
    "                 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'isn‚Äôt', 'it', 'it‚Äôd', 'it‚Äôll',\\\n",
    "                 'it‚Äôs', 'its', 'itself', 'just', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'last', 'lately', 'later', 'latter', 'latterly',\\\n",
    "                 'least', 'less', 'lest', 'let', 'let‚Äôs', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'mainly', 'many', 'may', 'maybe',\\\n",
    "                 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary',\\\n",
    "                 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere',\\\n",
    "                 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own',\\\n",
    "                 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'que', 'quite', 'qv', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right',\\\n",
    "                 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she',\\\n",
    "                 'should', 'shouldn‚Äôt', 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying',\\\n",
    "                 'still', 'sub', 'such', 'sup', 'sure', 't‚Äôs', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'that‚Äôs', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'there‚Äôs', 'thereafter', 'thereby',\\\n",
    "                 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'they‚Äôd', 'they‚Äôll', 'they‚Äôre', 'they‚Äôve', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards',\\\n",
    "                 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon',\\\n",
    "                 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'value', 'various', 'very', 'via', 'viz', 'vs', 'want', 'wants', 'was', 'wasn‚Äôt', 'way',\\\n",
    "                 'we', 'we‚Äôd', 'we‚Äôll', 'we‚Äôre', 'we‚Äôve', 'welcome', 'well', 'went', 'were', 'weren‚Äôt', 'what', 'what‚Äôs', 'whatever', 'when', 'whence', 'whenever', 'where', 'where‚Äôs', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither',\\\n",
    "                 'who', 'who‚Äôs', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'won‚Äôt', 'wonder', 'would', 'wouldn‚Äôt', 'yes', 'yet',\\\n",
    "                 'you', 'you‚Äôd', 'you‚Äôll', 'you‚Äôre', 'you‚Äôve', 'your', 'yours', 'yourself', 'yourselves', 'zero'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les mots qui n'expriment aucun sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104643062902788</td>\n",
       "      <td>@ambr_ncole they're tourists</td>\n",
       "      <td>ambrncole theyre tourists</td>\n",
       "      <td>[ambrncole, theyre, tourists]</td>\n",
       "      <td>[ambrncole, tourists]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329101613940797441</td>\n",
       "      <td>@PhallonXOXO proof you're doing it right üòå</td>\n",
       "      <td>PhallonXOXO proof youre doing it right üòå</td>\n",
       "      <td>[phallonxoxo, proof, youre, doing, it, right, üòå]</td>\n",
       "      <td>[phallonxoxo, proof, üòå]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328838299419627525</td>\n",
       "      <td>some of you hating...\\n\\nbut we see you Fleeti...</td>\n",
       "      <td>some of you hating\\n\\nbut we see you Fleeting üßê</td>\n",
       "      <td>[some, of, you, hating\\n\\nbut, we, see, you, f...</td>\n",
       "      <td>[hating\\n\\nbut, fleeting, üßê]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328684389388185600</td>\n",
       "      <td>That thing you didn‚Äôt Tweet but wanted to but ...</td>\n",
       "      <td>That thing you didn‚Äôt Tweet but wanted to but ...</td>\n",
       "      <td>[that, thing, you, didn‚Äôt, tweet, but, wanted,...</td>\n",
       "      <td>[thing, tweet, wanted, close, nah, \\n\\nwe, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1328426768009736192</td>\n",
       "      <td>@quakerraina this is art</td>\n",
       "      <td>quakerraina this is art</td>\n",
       "      <td>[quakerraina, this, is, art]</td>\n",
       "      <td>[quakerraina, art]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "5  2020-11-22  13:18  1329104643062902788   \n",
       "6  2020-11-22  13:18  1329101613940797441   \n",
       "7  2020-11-22  13:18  1328838299419627525   \n",
       "8  2020-11-22  13:18  1328684389388185600   \n",
       "9  2020-11-22  13:18  1328426768009736192   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "5                       @ambr_ncole they're tourists   \n",
       "6         @PhallonXOXO proof you're doing it right üòå   \n",
       "7  some of you hating...\\n\\nbut we see you Fleeti...   \n",
       "8  That thing you didn‚Äôt Tweet but wanted to but ...   \n",
       "9                           @quakerraina this is art   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "5                          ambrncole theyre tourists   \n",
       "6           PhallonXOXO proof youre doing it right üòå   \n",
       "7    some of you hating\\n\\nbut we see you Fleeting üßê   \n",
       "8  That thing you didn‚Äôt Tweet but wanted to but ...   \n",
       "9                            quakerraina this is art   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]   \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...   \n",
       "5                      [ambrncole, theyre, tourists]   \n",
       "6   [phallonxoxo, proof, youre, doing, it, right, üòå]   \n",
       "7  [some, of, you, hating\\n\\nbut, we, see, you, f...   \n",
       "8  [that, thing, you, didn‚Äôt, tweet, but, wanted,...   \n",
       "9                       [quakerraina, this, is, art]   \n",
       "\n",
       "                                       Tweet_nonstop  \n",
       "0                [rt, shesooosaddity, twitter, , rt]  \n",
       "1                                      [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replaced, good, m...  \n",
       "3                  [nethatguy, taking, oomf, fleets]  \n",
       "4               [jusjust, remember, dedicate, tweet]  \n",
       "5                              [ambrncole, tourists]  \n",
       "6                            [phallonxoxo, proof, üòå]  \n",
       "7                       [hating\\n\\nbut, fleeting, üßê]  \n",
       "8  [thing, tweet, wanted, close, nah, \\n\\nwe, pla...  \n",
       "9                                 [quakerraina, art]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tweet_df['Tweet_nonstop'] = tweet_df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK est une suite de biblioth√®ques de traitement de texte pour la classification, la tokenisation, la recherche de racines, le marquage, l'analyse et le raisonnement s√©mantique.\n",
    "On vas utiliser la biblioth√®que NLTK pour effectuer une analyse de chaque tweet et le transformer en un\n",
    "ensemble de mots en suivant les diff√©rentes √©tapes de base du processus NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming et Lammitization est une technique utilis√©e pour extraire la forme de base des mots en supprimant les affixes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "      <td>[rt, shesooosadd, twitter, , rt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replac, good, morn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleets]</td>\n",
       "      <td>[nethatguy, take, oomf, fleet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "      <td>[jusjust, rememb, dedic, tweet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]   \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...   \n",
       "\n",
       "                                       Tweet_nonstop  \\\n",
       "0                [rt, shesooosaddity, twitter, , rt]   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3                  [nethatguy, taking, oomf, fleets]   \n",
       "4               [jusjust, remember, dedicate, tweet]   \n",
       "\n",
       "                                      Tweet_stemmed  \n",
       "0                  [rt, shesooosadd, twitter, , rt]  \n",
       "1                                     [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replac, good, morn]  \n",
       "3                    [nethatguy, take, oomf, fleet]  \n",
       "4                   [jusjust, rememb, dedic, tweet]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweet_df['Tweet_stemmed'] = tweet_df['Tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329561340596391936</td>\n",
       "      <td>RT @shesooosaddity: if you had a twitter befor...</td>\n",
       "      <td>RT shesooosaddity if you had a twitter before ...</td>\n",
       "      <td>[rt, shesooosaddity, if, you, had, a, twitter,...</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "      <td>[rt, shesooosadd, twitter, , rt]</td>\n",
       "      <td>[rt, shesooosaddity, twitter, , rt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329217044391342082</td>\n",
       "      <td>@CloudNaii 40404</td>\n",
       "      <td>CloudNaii</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "      <td>[cloudnaii, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329216472711827458</td>\n",
       "      <td>@issahairplug drink water replaced good morning</td>\n",
       "      <td>issahairplug drink water replaced good morning</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "      <td>[issahairplug, drink, water, replac, good, morn]</td>\n",
       "      <td>[issahairplug, drink, water, replaced, good, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329107688916135936</td>\n",
       "      <td>@Ne_ThatGuy we're taking oomf to the Fleets</td>\n",
       "      <td>NeThatGuy were taking oomf to the Fleets</td>\n",
       "      <td>[nethatguy, were, taking, oomf, to, the, fleets]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleets]</td>\n",
       "      <td>[nethatguy, take, oomf, fleet]</td>\n",
       "      <td>[nethatguy, taking, oomf, fleet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-22  13:18</td>\n",
       "      <td>1329104797727940612</td>\n",
       "      <td>@_JusJust_ remember \"I dedicate my 500th Tweet...</td>\n",
       "      <td>JusJust remember I dedicate my th Tweet to</td>\n",
       "      <td>[jusjust, remember, i, dedicate, my, th, tweet...</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "      <td>[jusjust, rememb, dedic, tweet]</td>\n",
       "      <td>[jusjust, remember, dedicate, tweet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date              TweetId  \\\n",
       "0  2020-11-22  13:18  1329561340596391936   \n",
       "1  2020-11-22  13:18  1329217044391342082   \n",
       "2  2020-11-22  13:18  1329216472711827458   \n",
       "3  2020-11-22  13:18  1329107688916135936   \n",
       "4  2020-11-22  13:18  1329104797727940612   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  RT @shesooosaddity: if you had a twitter befor...   \n",
       "1                                   @CloudNaii 40404   \n",
       "2    @issahairplug drink water replaced good morning   \n",
       "3        @Ne_ThatGuy we're taking oomf to the Fleets   \n",
       "4  @_JusJust_ remember \"I dedicate my 500th Tweet...   \n",
       "\n",
       "                                         Tweet_punct  \\\n",
       "0  RT shesooosaddity if you had a twitter before ...   \n",
       "1                                         CloudNaii    \n",
       "2     issahairplug drink water replaced good morning   \n",
       "3           NeThatGuy were taking oomf to the Fleets   \n",
       "4         JusJust remember I dedicate my th Tweet to   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [rt, shesooosaddity, if, you, had, a, twitter,...   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3   [nethatguy, were, taking, oomf, to, the, fleets]   \n",
       "4  [jusjust, remember, i, dedicate, my, th, tweet...   \n",
       "\n",
       "                                       Tweet_nonstop  \\\n",
       "0                [rt, shesooosaddity, twitter, , rt]   \n",
       "1                                      [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replaced, good, m...   \n",
       "3                  [nethatguy, taking, oomf, fleets]   \n",
       "4               [jusjust, remember, dedicate, tweet]   \n",
       "\n",
       "                                      Tweet_stemmed  \\\n",
       "0                  [rt, shesooosadd, twitter, , rt]   \n",
       "1                                     [cloudnaii, ]   \n",
       "2  [issahairplug, drink, water, replac, good, morn]   \n",
       "3                    [nethatguy, take, oomf, fleet]   \n",
       "4                   [jusjust, rememb, dedic, tweet]   \n",
       "\n",
       "                                    Tweet_lemmatized  \n",
       "0                [rt, shesooosaddity, twitter, , rt]  \n",
       "1                                      [cloudnaii, ]  \n",
       "2  [issahairplug, drink, water, replaced, good, m...  \n",
       "3                   [nethatguy, taking, oomf, fleet]  \n",
       "4               [jusjust, remember, dedicate, tweet]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweet_df['Tweet_lemmatized'] = tweet_df['Tweet_nonstop'].apply(lambda x: lemmatizer(x))\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Les donn√©es apr√®s le pr√©traitement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      [rt, shesooosaddity, twitter, , rt]\n",
       "1                                            [cloudnaii, ]\n",
       "2        [issahairplug, drink, water, replaced, good, m...\n",
       "3                         [nethatguy, taking, oomf, fleet]\n",
       "4                     [jusjust, remember, dedicate, tweet]\n",
       "                               ...                        \n",
       "12885               [themegaboi, keeping, brain, thinking]\n",
       "12886    [guillaumetc, hamillhimself, chrisevans, combo...\n",
       "12887                                   [ksjize, dogrates]\n",
       "12888               [insomniacookies, cc, mecookiemonster]\n",
       "12889    [mnoir, amp, guaranteed, good, morning, good, ...\n",
       "Name: Tweet_lemmatized, Length: 12890, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.Tweet_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On va enregistrer notre dataset pr√©trait√© dans un nouveau fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.Tweet_lemmatized.to_csv('Datasets/new_cleaned_tweets.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (12890, 1)\n",
      "Columns are: Index(['Tweet_lemmatized'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12890 entries, 0 to 12889\n",
      "Data columns (total 1 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Tweet_lemmatized  12890 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 100.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['rt', 'shesooosaddity', 'twitter', '', 'rt']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cloudnaii', '']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['issahairplug', 'drink', 'water', 'replaced',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nethatguy', 'taking', 'oomf', 'fleet']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['jusjust', 'remember', 'dedicate', 'tweet']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Tweet_lemmatized\n",
       "0      ['rt', 'shesooosaddity', 'twitter', '', 'rt']\n",
       "1                                  ['cloudnaii', '']\n",
       "2  ['issahairplug', 'drink', 'water', 'replaced',...\n",
       "3           ['nethatguy', 'taking', 'oomf', 'fleet']\n",
       "4       ['jusjust', 'remember', 'dedicate', 'tweet']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affichage\n",
    "new_tweet_df= pd.read_csv('Datasets/new_cleaned_tweets.csv')\n",
    "print('Dataset size:',new_tweet_df.shape)\n",
    "print('Columns are:',new_tweet_df.columns)\n",
    "new_tweet_df.info()\n",
    "new_tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation\n",
    "Les donn√©es nettoy√©es devient sur une seule ligne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4591)\t2\n",
      "  (0, 4803)\t1\n",
      "  (0, 5616)\t1\n",
      "  (1, 899)\t1\n",
      "  (2, 2722)\t1\n",
      "  (2, 1340)\t1\n",
      "  (2, 5816)\t1\n",
      "  (2, 4480)\t1\n",
      "  (2, 1859)\t1\n",
      "  (2, 3637)\t1\n",
      "  (3, 3772)\t1\n",
      "  (3, 5242)\t1\n",
      "  (3, 4027)\t1\n",
      "  (3, 1687)\t1\n",
      "  (4, 2958)\t1\n",
      "  (4, 4468)\t1\n",
      "  (4, 1185)\t1\n",
      "  (4, 5603)\t1\n",
      "  (5, 172)\t1\n",
      "  (5, 5526)\t1\n",
      "  (6, 4167)\t1\n",
      "  (6, 4307)\t1\n",
      "  (7, 1994)\t1\n",
      "  (7, 3745)\t1\n",
      "  (7, 1688)\t1\n",
      "  :\t:\n",
      "  (12882, 3461)\t1\n",
      "  (12883, 2587)\t1\n",
      "  (12884, 369)\t1\n",
      "  (12884, 3607)\t1\n",
      "  (12885, 597)\t1\n",
      "  (12885, 5407)\t1\n",
      "  (12885, 3008)\t1\n",
      "  (12885, 5362)\t1\n",
      "  (12886, 1931)\t1\n",
      "  (12886, 1955)\t1\n",
      "  (12886, 844)\t1\n",
      "  (12886, 941)\t1\n",
      "  (12886, 5863)\t1\n",
      "  (12887, 1297)\t1\n",
      "  (12887, 3092)\t1\n",
      "  (12888, 3474)\t1\n",
      "  (12888, 758)\t1\n",
      "  (12888, 2680)\t1\n",
      "  (12889, 1859)\t2\n",
      "  (12889, 3637)\t1\n",
      "  (12889, 5603)\t1\n",
      "  (12889, 3865)\t1\n",
      "  (12889, 179)\t1\n",
      "  (12889, 3598)\t1\n",
      "  (12889, 1925)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X=cv.fit_transform(new_tweet_df.Tweet_lemmatized)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des tweets\n",
    "\n",
    "On va utiliser l‚Äôalgorithme K-Means pour classer les Tweets en 30 classes, qui est un algorithme non supervis√©  de clustering, populaire en Machine Learning qui peuvent classer chaque tweet √† une cat√©gorie particuli√®re ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:fitting model for 3 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 4 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 5 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 6 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 7 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 8 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 9 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 10 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 11 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 12 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 13 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n",
      "WARNING:root:fitting model for 14 clusters\n",
      "C:\\Users\\PC\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "seed = 42\n",
    "\n",
    "ks = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "# track a couple of metrics\n",
    "sil_scores = []\n",
    "result= []\n",
    "\n",
    "# fit the models, save the evaluation metrics from each run\n",
    "for k in ks:\n",
    "    logging.warning('fitting model for {} clusters'.format(k))\n",
    "    Kmeans = KMeans(n_clusters=k, n_jobs=-1, random_state=seed)\n",
    "    Kmeans.fit(X)\n",
    "    labels = Kmeans.labels_\n",
    "    #sil_scores.append(silhouette_score(bio_matrix, labels))\n",
    "    result.append(Kmeans.inertia_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, result, 'o--')\n",
    "plt.ylabel('result')\n",
    "plt.title('kmeans parameter search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks,result)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('word per cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k=30\n",
    "Kmeans=KMeans(n_clusters=true_k,init='k-means++',n_init=1)\n",
    "Kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = Kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    print(\"-----------------------\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour chaque cluster on va afficher un seul tweet \n",
    "On va afficher le tweet qui a le plus grand score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "while i<28:\n",
    "    while True: \n",
    "        Y=cv.transform([new_tweet_df.Tweet_lemmatized[j]])\n",
    "        prediction=Kmeans.predict(Y)\n",
    "        if i == prediction:\n",
    "            print(\"Tweet of cluster \"+str(prediction)+\" : \"+tweet_df.Tweet[i])\n",
    "            print (\"-----------------------------------------------\")\n",
    "            print(\"\\n\")\n",
    "            j=0\n",
    "            break\n",
    "        j+=1\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
